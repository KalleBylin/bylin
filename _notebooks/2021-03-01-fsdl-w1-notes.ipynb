{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Stack Deep Learning - W1 Notes\n",
    "> A short summary of DL fundamentals.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [deep-learning]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This notebook contains my notes for [FSDL Week 1: DL Fundamentals](https://fullstackdeeplearning.com/spring2021/lecture-1/).\n",
    "\n",
    "Note: I have added additional information in addition to the lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a perceptron?\n",
    "\n",
    "- Perceptrons were originally brain models created to understand how the brain works. A perceptron as we know it encodes several principles about how the brain works and then evolved into an algorithm for supervised binary classification.\n",
    "\n",
    "In the 1960's, Frank Rosenblatt published the book `Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms`. It is curious that this fundamental block for AI was, in the author's mind, a tool for understanding the human brain and not for pattern recognition (even though he encouraged this use as well):\n",
    "\n",
    "> For this writer, the perceptron program is _not_ primarily concerned with the invention of devices for \"artificial intelligence\", but rather with investigating the physical structures and neurodynamic principles which underlie \"natural intelligence\". *A perceptron is first and foremost a brain model, not an invention for pattern recognition* [emphasis added]\" (p. viii). \n",
    "\n",
    "In other words, the perceptron is actually a simplification and abstraction which has allowed us to discover principles for how the brain works. These same principles were then also used to create pattern recognition machines.\n",
    "\n",
    "Rosenblatt explicitly recognizes that his model is a direct descendant of the model created by McCulloch and Pitts, and influenced by the theories of Hebb and Hayek. \n",
    "\n",
    "Main components of a perceptron in Rosenblatt's book:\n",
    "- **Environment**: The environment generates the information that is initially passed on to the perceptron.\n",
    "\n",
    "\n",
    "- **Signal generating units**: Each unit receives a signal and generates an output signal.\n",
    "\n",
    "\n",
    "- **Signal propagation functions**: These are rules that define how signals are generated and transmitted.\n",
    "\n",
    "\n",
    "- **Memory functions**: These are rules that define how properties of the perceptron can be changed in response to certain activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of a single neuron evolves from the ideas above.\n",
    "\n",
    "A neuron takes values from its environment (x1, x2, x3) and each of these get multiplied by a stored parameter (w1, w2, w3). The sum of each of these operations is then passed through an activation function.\n",
    "\n",
    "In other words, it's as if we are trying to pass a signal through the neuron and all of these components work together to establish how the signal is transmitted. \n",
    "\n",
    "In the example below, we are randomly initializing the parameters. The activation function is a step-function with a threshold of 5. This means that the signal is only passed on as a unitary value if it is larger than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 is equal to: 1\n",
      "W2 is equal to: 6\n",
      "W3 is equal to: 9\n",
      "\n",
      "Output: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w1 = np.random.randint(1,10)\n",
    "w2 = np.random.randint(1,10)\n",
    "w3 = np.random.randint(1,10)\n",
    "\n",
    "print(f'W1 is equal to: {w1}')\n",
    "print(f'W2 is equal to: {w2}')\n",
    "print(f'W3 is equal to: {w3}\\n')\n",
    "\n",
    "x1 = 5\n",
    "x2 = 3\n",
    "x3 = 2\n",
    "\n",
    "b = 2\n",
    "\n",
    "activation_function = lambda x: 1 if x > 5 else 0\n",
    "\n",
    "output = activation_function(x1*w1 + x2*w2 + x3*w3)\n",
    "\n",
    "print('Output:', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an activation function?\n",
    "\n",
    "Going back to Rosenblatt's book, activation functions are essentially signal propagation functions. \n",
    "\n",
    "When a neuron receives a signal, the activation function decides if the signal is passed on and how strong the output signal becomes. \n",
    "\n",
    "We already learned about the step function. This activation is often not ideal for multiple reasons.\n",
    "\n",
    "First of all, the result is binary. But sometimes we are more interested in also knowing the degree of certainty, so I probability might be better.\n",
    "\n",
    "We might also want to have a wider range of values. When predicting age, for example, binary values of 0 or 1 will be of little value. \n",
    "\n",
    "A lot of different activation functions have been developed by researchers. Three common activation functions mentioned in the lecture are:\n",
    "\n",
    "**Sigmoid function**\n",
    "Has a great property of having outputs between 0 and 1 and therefor can be interpreted as probabilities. The function is also smooth and easy to differentiate which makes learning easier.\n",
    "\n",
    "It can be problematic when the input signal has very large positive or negative values. At those points the derivative is very close to 0 and learning becomes very slow.\n",
    "\n",
    "**Hyperbolic function**\n",
    "This is another smooth function with a range of values between -1 and 1. This is interesting because sometimes a signal might have a reverse effect on the output and the hyperbolic function allows us to include this type of relationship in the network. \n",
    "\n",
    "**Rectified linear unit (ReLU)**\n",
    "This function is very simple aned fast to compute. This allows us to work with larger and more complex models that, given enough data, can produce better results overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is universality?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
