{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthy Online Controlled Experiments\n",
    "> A Practical Guide to A/B Testing\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [books]\n",
    "- image: img/controlled experiments.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/controlled experiments.jpg\" alt=\"ab-testing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 The book in 3 sentences \n",
    "\n",
    "[Still reading the book] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎨 Impressions\n",
    "\n",
    "[Still reading the book] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ☘️ How the book changed me\n",
    "\n",
    "[Still reading the book] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✍️ My top 3 quotes\n",
    "\n",
    "[Still reading the book] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📒 Summary + Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introductory topics for everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction and motivation\n",
    "\n",
    "The book starts with an anecdote from Bing where an employee had proposed a new idea for how ad headlines should be displayed. The basic idea was to make the title longer by combining it with text from the first line under the title. Nobody seemed to think much of this idea and it was stashed into the backlog of experiments for six months before a software engineer decided to test the idea.\n",
    "\n",
    "This simple change ended up increasing revenue by 12%, which translated to more than $100 million dollars in the US alone annually without hurting other important metrics.\n",
    "\n",
    "A few takeaways from this event are that it's very hard to predict the value of an idea (this one ended up being forgotten during 6 months) and small changes can have huge impacts. For this reason it is important to have infrastructure in place that makes experimenting very cheap. Bing runs more than 10.000 experiments per year and the great majority don't have this kind of results.\n",
    "\n",
    "There are many types of **controlled experiments**. The main focus of this book is on a specific type of controlled experiment called an A/B test. The basic idea is to split users randomly between a control version and a variant.\n",
    "\n",
    "Another very important concept is the **Overall Evaluation Criterion (OEC)**. This could be a single metric or a combination of different metrics. The most important thing to remember is that the OEC has to be measurable in the short-term and believed to have a causal relationship with long-term success according to established strategic objectives.\n",
    "\n",
    "Think about the ad headline example. If the success metric was only revenue, it wouldn't take long before the whole site was full of ads. This could definitely increase revenue in the short-term, but would probably turn users off in the long-term causing them to switch over to friendlier alternatives. \n",
    "\n",
    "When designing a controlled experiment, it is common to choose a **parameter** or set of parameters. These are variables that we think are going to have an impact on the OEC. These parameters are going to take on different values for each variant. \n",
    "\n",
    "It is also **important to thoughtfully set up the randomization process**. This is the secret sauce of controlled experiments. The basic intuition behind this is that if there is no other factor influencing assignment apart from proper randomization, then on average we would expect both groups to be similar on all factors except the parameter we are changing. This means that any observed effect on the OEC should with very high probability be an effect of that change and nothing else.   \n",
    "\n",
    "This leads us into a conversation around **correlation, causality and trustworthiness**. The authors present a slightly silly but true example by sharing that the number of error messages has a strong inverse correlation with churn. Should we then increase the number of error messages or intentionally add bugs to the code of Office 365? Obviously not, because error messages don't cause lower churn. It turns out that power users tend to churn less and bump into more error messages just because they use the product more than other users. \n",
    "\n",
    "> \"We believe online controlled experiments are the best scientific way to establish causality with high probability\"\n",
    "\n",
    "Of course, it's not always possible to run a controlled experiment. For example, we can't create two alternate realities where we buy another company in one reality and we don't buy the other company in the other reality to see which option is better. \n",
    "\n",
    "The authors list a series of **ingredients that need to be present to be able to run a controlled experiment**:\n",
    "\n",
    "\n",
    "1. **Experimental units**, like users, that can be assigned to each variant without interference where users in one group could affect the actions of the users of the other group.\n",
    "\n",
    "2. **Enough experimental units**. The authors recommend to be at least in the thousands. The more units we have the more granular we can be in our experiments and discover smaller effects that are harder to detect.  \n",
    "\n",
    "3. **Key metrics** that are easy to understand, agreed upon and can be measured easily. If possible, the OEC should be used. A proxy metric can be used if the main metric is too difficult or slow to measure.\n",
    "\n",
    "4. **Easy changes**. The harder it is to create a variant, the harder it becomes to run a controlled experiment. This is one of the reasons software has become an ideal playground for running controlled experiments. \n",
    "\n",
    "\n",
    "The authors also present three tenets or key beliefs that any organization needs to run online controlled experiments:\n",
    "\n",
    "\n",
    "1. **The organization wants to make data-driven decisions and has formalized an OEC**\n",
    "\n",
    "Obviously, most people will say that data-driven decision-making is important for them. Still, in practice it is very common to plan, execute and declare success based on how much of the plan was accomplished while ignoring the impact on key metrics.\n",
    "\n",
    "In contrast, for data-driven decision-making to exist we need a clearly defined OEC (or set of OECs) that is measurable in the short term (1 to 2 weeks) and predictive of long-term success. This way we can quickly evaluate if what we are doing tactically and strategically is actually having a real impact on our OEC.\n",
    "\n",
    "\n",
    "2. **The organization is willing to invest in the infrastructure needed to run and test controlled experiments so that results are trustworthy**\n",
    "\n",
    "In some domains, like healthcare, it can be considered unethical or illegal to run certain controlled experiments. In other domains, like hardware production, changes can be slow and expensive.\n",
    "\n",
    "In general, software is a great option because it tends to be relatively easy to randomly split users between variants, log the results and iteratively add changes to the software. Still, the organization still has to be willing to invest in setting up the necessary tests and processes so that results are trustworthy at scale. \n",
    "\n",
    "\n",
    "3. **The organization recongizes that it is poor at assessing the value of ideas**\n",
    "\n",
    "Based on quotes from experts at Microsoft, Google, Netflix, Slack and others, the authors claim that most teams see experiment success rates of 10-33\\%, with most of them being on the lower end. In other words, we can expect 70-90\\% of our work to be thrown away due to poor or even negative results. \n",
    "\n",
    "> \"Most who have run controlled experiments in customer-facing websites and applications have experienced this humbling reality: *we are poor at assessing the value of ideas*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Running and analyzing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
