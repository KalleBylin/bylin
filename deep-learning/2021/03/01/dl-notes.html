<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deep Learning - Notes | Books &amp; Data</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Deep Learning - Notes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A short summary of DL fundamentals." />
<meta property="og:description" content="A short summary of DL fundamentals." />
<link rel="canonical" href="https://kallebylin.github.io/bylin/deep-learning/2021/03/01/dl-notes.html" />
<meta property="og:url" content="https://kallebylin.github.io/bylin/deep-learning/2021/03/01/dl-notes.html" />
<meta property="og:site_name" content="Books &amp; Data" />
<meta property="og:image" content="https://kallebylin.github.io/bylin/images/chart-preview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-01T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://kallebylin.github.io/bylin/deep-learning/2021/03/01/dl-notes.html","@type":"BlogPosting","headline":"Deep Learning - Notes","dateModified":"2021-03-01T00:00:00-06:00","datePublished":"2021-03-01T00:00:00-06:00","image":"https://kallebylin.github.io/bylin/images/chart-preview.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://kallebylin.github.io/bylin/deep-learning/2021/03/01/dl-notes.html"},"description":"A short summary of DL fundamentals.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/bylin/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://kallebylin.github.io/bylin/feed.xml" title="Books &amp; Data" /><link rel="shortcut icon" type="image/x-icon" href="/bylin/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/bylin/">Books &amp; Data</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/bylin/about/">About Me</a><a class="page-link" href="/bylin/search/">Search</a><a class="page-link" href="/bylin/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Learning - Notes</h1><p class="page-description">A short summary of DL fundamentals.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-01T00:00:00-06:00" itemprop="datePublished">
        Mar 1, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      13 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/bylin/categories/#deep-learning">deep-learning</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/kallebylin/bylin/tree/master/_notebooks/2021-03-01-dl-notes.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/bylin/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/kallebylin/bylin/master?filepath=_notebooks%2F2021-03-01-dl-notes.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/bylin/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/kallebylin/bylin/blob/master/_notebooks/2021-03-01-dl-notes.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/bylin/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#About">About </a>
<ul>
<li class="toc-entry toc-h2"><a href="#What-is-a-perceptron?">What is a perceptron? </a></li>
<li class="toc-entry toc-h2"><a href="#What-is-an-activation-function?">What is an activation function? </a></li>
<li class="toc-entry toc-h2"><a href="#What-is-a-neural-network?">What is a neural network? </a></li>
<li class="toc-entry toc-h2"><a href="#What-is-universality?">What is universality? </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-03-01-dl-notes.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="About">
<a class="anchor" href="#About" aria-hidden="true"><span class="octicon octicon-link"></span></a>About<a class="anchor-link" href="#About"> </a>
</h1>
<p>This notebook contains my notes for introductory DL concepts.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-a-perceptron?">
<a class="anchor" href="#What-is-a-perceptron?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is a perceptron?<a class="anchor-link" href="#What-is-a-perceptron?"> </a>
</h2>
<ul>
<li>Perceptrons were originally brain models created to understand how the brain works. A perceptron as we know it encodes several principles about how the brain works and then evolved into an algorithm for supervised binary classification.</li>
</ul>
<p>In the 1960's, Frank Rosenblatt published the book <code>Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</code>. It is curious that this fundamental block for AI was, in the author's mind, a tool for understanding the human brain and not for pattern recognition (even though he encouraged this use as well):</p>
<blockquote>
<p>For this writer, the perceptron program is <em>not</em> primarily concerned with the invention of devices for "artificial intelligence", but rather with investigating the physical structures and neurodynamic principles which underlie "natural intelligence". <em>A perceptron is first and foremost a brain model, not an invention for pattern recognition</em> [emphasis added]" (p. viii).</p>
</blockquote>
<p>In other words, the perceptron is actually a simplification and abstraction which has allowed us to discover principles for how the brain works. These same principles were then also used to create pattern recognition machines.</p>
<p>Rosenblatt explicitly recognizes that his model is a direct descendant of the model created by McCulloch and Pitts, and influenced by the theories of Hebb and Hayek.</p>
<p>Main components of a perceptron in Rosenblatt's book:- <strong>Environment</strong>: The environment generates the information that is initially passed on to the perceptron.</p>
<ul>
<li>
<strong>Signal generating units</strong>: Each unit receives a signal and generates an output signal.</li>
</ul>
<ul>
<li>
<strong>Signal propagation functions</strong>: These are rules that define how signals are generated and transmitted.</li>
</ul>
<ul>
<li>
<strong>Memory functions</strong>: These are rules that define how properties of the perceptron can be changed in response to certain activity.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The definition of a single neuron evolves from the ideas above.</p>
<p>A neuron takes values from its environment (e.g. x1, x2, x3) and each of these gets multiplied by a stored parameter (e.g. w1, w2, w3). The sum of each of these operations is then passed through an activation function.</p>
<p>In other words, it's as if we are trying to pass a signal through the neuron and all of these components work together to establish how the signal is transmitted.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can imagine three old batteries used to turn on a machine. Turning it on with too little energy could cause it to break, so we check the current before passing it on to the machine.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/bylin/images/copied_from_nb/fastcore_imgs/perceptron_batteries.png" alt="Perceptron"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the example below, we are randomly initializing the parameters. The activation function is a step-function with a threshold of 4. This means that the signal is only passed on as a unitary value if it is larger than or equal to the threshold.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span><span class="o">*</span><span class="mi">2</span> <span class="c1"># generates random floats between 0 and 2</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span><span class="o">*</span><span class="mi">2</span>
<span class="n">w3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span><span class="o">*</span><span class="mi">2</span> 

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'W1 is equal to: </span><span class="si">{</span><span class="n">w1</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'W2 is equal to: </span><span class="si">{</span><span class="n">w2</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'W3 is equal to: </span><span class="si">{</span><span class="n">w3</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

<span class="n">x1</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">x2</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">x3</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">activation_function</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span><span class="mi">4</span> <span class="k">else</span> <span class="mi">0</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">activation_function</span><span class="p">(</span><span class="n">x1</span><span class="o">*</span><span class="n">w1</span> <span class="o">+</span> <span class="n">x2</span><span class="o">*</span><span class="n">w2</span> <span class="o">+</span> <span class="n">x3</span><span class="o">*</span><span class="n">w3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Output:'</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

<span class="k">if</span> <span class="n">output</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'The machine is on!'</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Not enough energy to turn on the machine :('</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>W1 is equal to: 0.6321002815675523
W2 is equal to: 1.6995522179113192
W3 is equal to: 1.2072466687862997

Output: 1
The machine is on!
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-an-activation-function?">
<a class="anchor" href="#What-is-an-activation-function?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is an activation function?<a class="anchor-link" href="#What-is-an-activation-function?"> </a>
</h2>
<p>Going back to Rosenblatt's book, activation functions are essentially signal propagation functions.</p>
<p>When a neuron receives a signal, the activation function decides if the signal is passed on and how strong the output signal becomes.</p>
<p>We already learned about the step function. This activation is often not ideal for multiple reasons.</p>
<p>First of all, the result is binary. But sometimes we are more interested in also knowing the degree of certainty, so I probability might be better.</p>
<p>We might also want to have a wider range of values. When predicting age, for example, binary values of 0 or 1 will be of little value.</p>
<p>A lot of different activation functions have been developed by researchers. Three common activation functions are:</p>
<p><strong>Sigmoid function</strong>
Has a great property of having outputs between 0 and 1 and therefore can be interpreted as probabilities. The function is also smooth and easy to differentiate which makes learning easier.</p>
<p>It can be problematic when the input signal has very large positive or negative values. At those points the derivative is very close to 0 and learning becomes very slow.</p>
<p><strong>Hyperbolic function</strong>
This is another smooth function with a range of values between -1 and 1. This is interesting because sometimes a signal might have a reverse effect on the output and the hyperbolic function allows us to include this type of relationship in the network.</p>
<p><strong>Rectified linear unit (ReLU)</strong>
This function is very simple aned fast to compute. This allows us to work with larger and more complex models that, given enough data, can produce better results overall.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-a-neural-network?">
<a class="anchor" href="#What-is-a-neural-network?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is a neural network?<a class="anchor-link" href="#What-is-a-neural-network?"> </a>
</h2>
<ul>
<li>A neural network is a directed system of connected neurons which is capable of propagating a signal received from the environment and produces an output signal.</li>
</ul>
<p>The neurons are generally organized in different layers which allow us to break free from linearity.</p>
<p>Linearity means that we are also assuming monotonicity. In other words, an increase in an input value always increases the output value if the corresponding weight is positive (and will always decrease the output value if the weight is negative).</p>
<p>This works fine in examples like the one seen above where we turn on a lightbulb depending on how much energy we get from each battery.</p>
<p>But this makes less sense in other cases. For examples very pronounced sales spikes in an online marketplace followed by no sales at all could be indicative of fraud. A positive value for a feature describing this type of spikes should increase the probability of fraud.</p>
<p>But is this always the case?</p>
<p>What if we additionally know that the owner of the store is a web celeb (ç½‘çº¢) on Alibaba? In that case the spikes above are expected, and therefore a positive value for this feature should decrease the probability of fraud while any other behavior might be suspicious.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Let's build a neural network step by step:</strong></p>
<p>The simplest example is a single neuron with only one parameter which does not modify the signal in any way:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">nn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span><span class="o">*</span><span class="n">x</span>

<span class="nb">print</span><span class="p">(</span><span class="n">nn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>18
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Most processes we find in the real world are not this simple. We need to take into account multiple input features.</p>
<p>I love music, imagine I want to predict if I will like a particular song or not. We can quickly multiply each input value by its corresponding weight using a dot product.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">x1</span> <span class="o">=</span> <span class="mf">3.5</span> <span class="c1"># length of songs in minutes</span>
<span class="n">x2</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># binary value indicating if the genre is jazz or not</span>
<span class="n">x3</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># binary value indicating if the artist is Nicki Minaj or not</span>
<span class="n">x4</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># binary value indicating if the artist is Alan Walker</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">])</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>7.5
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What is interesting above is that the weights actually encode certain information about my taste in music. I really don't like when a song is too short and, in general, I'm not fond of music by Nicki Minaj (notice the negative weight). But I do love music by Alan Walker. Jazz is nice but I don't love it.</p>
<p>Now the real question is, how do we know the values of these weights. In this case, I used my deep expertise regarding my own music taste to set the weights. But we would usually be more interested in doing this for the users of our streaming platform at scale. Millions of people we have never and will probably never meet in person.</p>
<p>So, first we're going to change the output. We want to predict if a particular user is going to like a song or not. Using the sigmoid function we can directly compare the true values (0 or 1) to our predictions (floats between 0 and 1).</p>
<p>We will also initialize the weights randomly and print out an error value showing how close our prediction is to the true value:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span> 
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y</span><span class="o">-</span><span class="n">prediction</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Predicted value:'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Error:'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">x1</span> <span class="o">=</span> <span class="mf">3.5</span> <span class="c1"># length of songs in minutes</span>
<span class="n">x2</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># binary value indicating if the genre is jazz or not</span>
<span class="n">x3</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># binary value indicating if the artist is Nicki Minaj or not</span>
<span class="n">x4</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># binary value indicating if the artist is Alan Walker</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="o">-</span><span class="mi">1</span>

<span class="n">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Predicted value: -1.468
Error: 6.468
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's now time to start talking about learning or how our neural network is going to choose the best parameters during training to make accurate predictions.</p>
<p>We will follow a very simple framework of three steps:</p>
<ol>
<li>Predict</li>
<li>Compare </li>
<li>Learn</li>
</ol>
<p>This time we will add multiple iterations to our algorithm and we will do the three steps above during each iteration.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span> 
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'True value: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">y</span><span class="o">-</span><span class="n">prediction</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">'Predicted value:'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'Error:'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="n">error</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">W</span> <span class="o">+=</span> <span class="mf">0.01</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">W</span> <span class="o">-=</span> <span class="mf">0.01</span>  

<span class="n">x1</span> <span class="o">=</span> <span class="mf">3.5</span> <span class="c1"># length of songs in minutes</span>
<span class="n">x2</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># binary value indicating if the genre is jazz or not</span>
<span class="n">x3</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># binary value indicating if the artist is Nicki Minaj or not</span>
<span class="n">x4</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># binary value indicating if the artist is Alan Walker</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="o">-</span><span class="mi">1</span>

<span class="n">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>True value: 1

Predicted value: 0.116
Error: 0.884
Predicted value: 0.161
Error: 0.839
Predicted value: 0.206
Error: 0.794
Predicted value: 0.251
Error: 0.749
Predicted value: 0.296
Error: 0.704
Predicted value: 0.341
Error: 0.659
Predicted value: 0.386
Error: 0.614
Predicted value: 0.431
Error: 0.569
Predicted value: 0.476
Error: 0.524
Predicted value: 0.521
Error: 0.479
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we are doing hot &amp; cold learning. After each prediction we are making a comparison just like in the classical game so that we get "hotter" or closer to the true answer after each guess.</p>
<p>The way we are calculating the error makes it negative if it was too high or positive if our guess was too low.</p>
<p>We can now be a bit smarter in the way we learn. Instead of guessing and trying to jiggle the output to both sides, we can use the gradient to guide us:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span> 
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'True values: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">mse</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">predictions</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Mean Squared Error:'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        
        <span class="n">dW</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">W</span> <span class="o">-=</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">dW</span>
          
    <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">W:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Final predictions:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>True values: [[1 0 0 1]]

Mean Squared Error: 0.234
Mean Squared Error: 0.132
Mean Squared Error: 0.078
Mean Squared Error: 0.046
Mean Squared Error: 0.028
Mean Squared Error: 0.017
Mean Squared Error: 0.01
Mean Squared Error: 0.006
Mean Squared Error: 0.004
Mean Squared Error: 0.002

W: [[0.927 0.073]]

Final predictions: [[0.92 0.07 0.   1.  ]]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This version led us quickly very close to the correct answers. But we now have another problem. The previous problem was very easy to learn because one of the features had a direct 1-on-1 relationship with an output. A simple linear function was capable of leveraging that feature.</p>
<p>Let's look at the next example:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>True values: [[1 1 0 0]]

Mean Squared Error: 0.863
Mean Squared Error: 0.572
Mean Squared Error: 0.408
Mean Squared Error: 0.311
Mean Squared Error: 0.254
Mean Squared Error: 0.219
Mean Squared Error: 0.198
Mean Squared Error: 0.186
Mean Squared Error: 0.178
Mean Squared Error: 0.174

W: [[0.462 0.204]]

Final predictions: [[0.47 0.2  0.   0.67]]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>MSE might not be extremely high but the answers are terrible. This is because the new problem we're working with cannot be solved with a simple linear function. So, to solve this we can add a hidden layer with an activation function (more on why this works can be found further down):</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="n">x</span>

<span class="k">def</span> <span class="nf">relu_prime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">&gt;</span><span class="mi">0</span>

<span class="k">def</span> <span class="nf">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span> 
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'True values: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
        <span class="n">a1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
        
        <span class="n">z2</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span> <span class="c1"># predictions</span>
        <span class="n">mse</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">z2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'Mean Squared Error:'</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        
        <span class="n">dZ2</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">z2</span><span class="p">)</span>
        <span class="n">dW2</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ2</span><span class="p">)</span>
        <span class="n">W2</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dW2</span>
        
        <span class="n">dZ1</span> <span class="o">=</span> <span class="p">(</span><span class="n">dZ2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">*</span><span class="n">relu_prime</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
        <span class="n">dW1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ1</span><span class="p">)</span>
        <span class="n">W1</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="n">dW1</span> <span class="c1">#(1/n)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">W1:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'W2:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Final predictions:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">z2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>

<span class="n">nn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>True values: [[1]
 [1]
 [0]
 [0]]

Mean Squared Error: 1.12
Mean Squared Error: 0.284
Mean Squared Error: 0.133
Mean Squared Error: 0.078
Mean Squared Error: 0.052
Mean Squared Error: 0.037
Mean Squared Error: 0.027
Mean Squared Error: 0.021
Mean Squared Error: 0.016
Mean Squared Error: 0.012

W1: [[-0.417 -0.056 -2.136  0.599]
 [-1.793 -0.842  0.847 -1.302]]
W2: [[-1.058]
 [-0.909]
 [ 0.876]
 [ 1.738]]

Final predictions: [[1.04]
 [0.74]
 [0.  ]
 [0.  ]]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-is-universality?">
<a class="anchor" href="#What-is-universality?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is universality?<a class="anchor-link" href="#What-is-universality?"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Universality is the ability to compute any arbitrary function.</li>
</ul>
<p>It has been proven that any function we can think of can be approximated by a neural network with at least two layers.</p>
<p><a href="http://neuralnetworksanddeeplearning.com/chap4.html#two_caveats">Michael Nielsen clarifies</a> two caveats related to this idea of universality:</p>
<ol>
<li>
<strong>Universality doesn't mean that the neural network is guaranteed to be exact</strong>, instead we are approximating the function and the more neurons we add the closer it gets to the true function. This means that for any function <em>f(X)</em> and any error threshold we define we can always find a neural network with output <em>g(x)</em> that satisfies: </li>
</ol>
<p>
$$ |g(x) - f(x)| &lt; \epsilon $$
</p>
<ol>
<li>
<strong>Neural networks approximate continuous functions</strong>. Functions that are not continuous with sharp and sudden jumps won't be approximated by a neural network as a general rule. But that doesn't mean that we can often use a continuous approximation that is good enough for our given purposes when trying to approximate a discontinuous function.</li>
</ol>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="kallebylin/bylin"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/bylin/deep-learning/2021/03/01/dl-notes.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/bylin/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/bylin/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/bylin/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A repo for my book reviews and notes.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/kallebylin" title="kallebylin"><svg class="svg-icon grey"><use xlink:href="/bylin/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/kallebylin" title="kallebylin"><svg class="svg-icon grey"><use xlink:href="/bylin/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
